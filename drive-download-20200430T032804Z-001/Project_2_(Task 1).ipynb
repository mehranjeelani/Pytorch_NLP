{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"colab":{"name":"Project_2_(Task 1).ipynb","provenance":[{"file_id":"1e6-Ez6iNmf64ZMqBFCteTyJhBR6SiKQm","timestamp":1582574955988}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"mBrdcAK28Gjr","colab_type":"text"},"source":["# Project 3: Text Classification in PyTorch\n","\n","## Instructions\n","\n","* All the tasks that you need to complete in this project are either coding tasks (mentioned inside the code cells of the notebook with `#TODO` notations) or theoretical questions that you need to answer by editing the markdown question cells.\n","* **Please make sure you read the [Notes](#Important-Notes) section carefully before you start the project.**\n","\n","## Introduction\n","This project deals with neural text classification using PyTorch. Text classification is the process of assigning tags or categories to text according to its content. It's one of the fundamental tasks in Natural Language Processing (NLP) with broad applications such as sentiment analysis, topic labeling, spam detection, and intent detection.\n","\n","Text classification algorithms are at the heart of a variety of software systems that process text data at scale. Email software uses text classification to determine whether incoming mail is sent to the inbox or filtered into the spam folder. Discussion forums use text classification to determine whether comments should be flagged as inappropriate.\n","\n","**_Example:_** A simple example of text classification would be Spam Classification. Consider the bunch of emails that you would receive in the your personal inbox if the email service provider did not have a spam filter algorithm. Because of the spam filter, spam emails get redirected to the Spam folder, while you receive only non-spam (\"_ham_\") emails in your inbox.\n","\n","![](http://blog.yhat.com/static/img/spam-filter.png)\n","\n","## Task\n","Here, we want you to focus on a specific type of text classification task, \"Document Classification into Topics\". It can be addressed as classifying text data or even large documents into separate discrete topics/genres of interest.\n","\n","\n","![](https://miro.medium.com/max/700/1*YWEqFeKKKzDiNWy5UfrTsg.png)\n","\n","In this project, you will be working on classifying given text data into discrete topics or genres. You are given a bunch of text data, each of which has a label attached. We ask you to learn why you think the contents of the documents have been given these labels based on their words. You need to create a neural classifier that is trained on this given information. Once you have a trained classifier, it should be able to predict the label for any new document or text data sample that is fed to it. The labels need not have any meaning to us, nor to you necessarily.\n","\n","## Data\n","There are various datasets that we can use for this purpose. This tutorial shows how to use the text classification datasets in the PyTorch library ``torchtext``. There are different datasets in this library like `AG_NEWS`, `SogouNews`, `DBpedia`, and others. This project will deal with training a supervised learning algorithm for classification using one of these datasets. In task 1 of this project, we will work with the `AG_NEWS` dataset.\n","\n","## Load Data\n","\n","A bag of **ngrams** feature is applied to capture some partial information about the local word order. In practice, bi-grams or tri-grams are applied to provide more benefits as word groups than only one word.\n","\n","**Example:**\n","\n","*\"I love Neural Networks\"*\n","* **Bi-grams:** \"I love\", \"love Neural\", \"Neural Networks\"\n","* **Tri-grams:** \"I love Neural\", \"love Neural Networks\"\n","\n","In the code below, we have loaded the `AG_NEWS` dataset from the ``torchtext.datasets.TextClassification`` package with bi-grams feature. The dataset supports the ngrams method. By setting ngrams to 2, the example text in the dataset will be a list of single words plus bi-grams string."]},{"cell_type":"code","metadata":{"id":"SbPLe8RV8GkG","colab_type":"code","outputId":"5e66ec1f-9523-4040-f099-880661343ea1","executionInfo":{"status":"ok","timestamp":1583238132165,"user_tz":-60,"elapsed":44453,"user":{"displayName":"kai glauber","photoUrl":"","userId":"07279436276219446584"}},"colab":{"base_uri":"https://localhost:8080/","height":247}},"source":["\"\"\"\n","Load the AG_NEWS dataset in bi-gram features format.\n","\"\"\"\n","\n","!pip install torchtext==0.4\n","\n","import torch\n","import torchtext\n","from torchtext.datasets import text_classification\n","import os\n","\n","from tqdm import tqdm\n","NGRAMS = 2\n","\n","if not os.path.isdir('./.data'):\n","    os.mkdir('./.data')\n","\n","train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n","    root='./.data', ngrams=NGRAMS, vocab=None)\n","\n","BATCH_SIZE = 16\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torchtext==0.4 in /usr/local/lib/python3.6/dist-packages (0.4.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (1.4.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (4.28.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (1.12.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (2.21.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (1.17.5)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (2019.11.28)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (3.0.4)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (1.24.3)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (2.8)\n"],"name":"stdout"},{"output_type":"stream","text":["120000lines [00:10, 11993.46lines/s]\n","120000lines [00:20, 5995.63lines/s]\n","7600lines [00:01, 6476.75lines/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"uJcbtMLB8Gk0","colab_type":"text"},"source":["## Model\n","\n","Our first simple model is composed of an [`EmbeddingBag`](https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag) layer and a linear layer.\n","\n","``EmbeddingBag`` computes the mean value of a “bag” of embeddings. The text entries here have different lengths. ``EmbeddingBag`` requires no padding here since the text lengths are saved in offsets. Additionally, since ``EmbeddingBag`` accumulates the average across the embeddings on the fly, ``EmbeddingBag`` can enhance the performance and memory efficiency to process a sequence of tensors."]},{"cell_type":"code","metadata":{"id":"f7cEiQGd8Gk7","colab_type":"code","colab":{}},"source":["# TODO: Import the necessary libraries\n","from torch import nn\n","\n","# TODO: Create a class TextClassifier. Remember that this class will be your model.\n","class TextClassifier(nn.Module):\n","\n","    # TODO: Define the __init__() method with proper parameters\n","    # (vocabulary size, dimensions of the embeddings, number of classes)\n","    def __init__(self, vocab_size, embed_dim, num_class):\n","        super().__init__()\n","        # TODO: define the embedding layer\n","        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim)\n","        # TODO: define the linear forward layer\n","        self.linear = nn.Linear(embed_dim, num_class)\n","        # TODO: Initialize weights\n","        self._initWeights()\n","\n","    # TODO: Define a method to initialize weights.\n","    def _initWeights(self):\n","        self._setWeightsAndBias(self.embedding)\n","        self._setWeightsAndBias(self.linear)\n","\n","        \n","    def _setWeightsAndBias(self, layer):\n","        # The weights should be random in the range of -0.5 to 0.5.\n","        stdv = 0.5\n","        layer.weight.data.uniform_(-stdv, stdv)\n","        # You can initialize bias values as zero.\n","        if hasattr(layer, \"bias\"):\n","            layer.bias.data.zero_()\n","\n","    \n","    # TODO: Define the forward function.\n","    def forward(self, inputs, offsets):\n","        # This should calculate the embeddings and return the linear layer\n","        # with calculated embedding values.\n","        x = self.embedding(inputs, offsets)\n","        x = self.linear(x)\n","        return x\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7RsDDP3n8GlI","colab_type":"text"},"source":["## Check your data before you proceed!\n","\n","Okay, so we know that we are using the `AG_NEWS` dataset in this project, but do you know what does the data contain? What is the format of the data? How many classes of data are there in this dataset? We do not know, yet. Let's find out!\n","\n","\n","## Question 1:\n","Create a new cell in this notebook and try to analyze the dataset that we loaded for you before. Report the following:\n","* Vocabulary size (VOCAB_SIZE)\n","* Number of classes (NUM_CLASS)\n","* Names of the classes\n","\n","\n","## Answer 1:"]},{"cell_type":"code","metadata":{"id":"1phtvJei8Glc","colab_type":"code","outputId":"68dea811-564f-471f-bdb1-d7eb0b0a25be","executionInfo":{"status":"ok","timestamp":1583238132169,"user_tz":-60,"elapsed":44364,"user":{"displayName":"kai glauber","photoUrl":"","userId":"07279436276219446584"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["def getVocabSize(dataset):\n","    return len(dataset.get_vocab())\n","\n","def getClasses(dataset):\n","    return dataset.get_labels()\n","\n","VOCAB_SIZE = getVocabSize(test_dataset)\n","CLASSES = getClasses(test_dataset)\n","NUM_CLASSES = len(CLASSES)\n","\n","\n","print(f\"VOCAB_SIZE: {VOCAB_SIZE}\")\n","print(f\"NUM_CLASSES: {NUM_CLASSES}\")\n","print(f\"CLASSES: {CLASSES}\")"],"execution_count":3,"outputs":[{"output_type":"stream","text":["VOCAB_SIZE: 1308844\n","NUM_CLASSES: 4\n","CLASSES: {0, 1, 2, 3}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fGi5_1aF8Gl6","colab_type":"text"},"source":["## Create an instance for your model\n","\n","Great! You have successfully completed a basic analysis of the data that you are going to work with. The vocab size is equal to the length of vocab (including single word and ngrams). The number of classes is equal to the number of labels. Copy paste the code statements you used in your analysis to complete the code below. Also, using these parameters, create an instance `model` of your text classifier `TextClassifier`."]},{"cell_type":"code","metadata":{"id":"i9fumaQr8GmD","colab_type":"code","colab":{}},"source":["'''\n","Paramters and model instance creation.\n","'''\n","\n","# TODO: Instantiate the Vocabulary size and the number of classes\n","# from the training dataset that we loaded for you.\n","\n","# Hint: Remember that these are PyTorch datasets. So, there should be \n","# readily available functions that you can use to save time. ;)\n","\n","VOCAB_SIZE = getVocabSize(train_dataset)\n","EMBED_DIM = 32\n","NUM_CLASS = len(getClasses(train_dataset))\n","\n","# TODO: Instantiate the model with the parameters you defined above. \n","# Remember to allocate it to your 'device' variable.\n","\n","model = TextClassifier(VOCAB_SIZE, EMBED_DIM, NUM_CLASS).to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PIXfOkGv8Gmo","colab_type":"text"},"source":["## Generate batch\n","\n","Since the text entries have different lengths, you need to create a custom function to generate data batches and offsets. This function should be passed to the ``collate_fn`` parameter in the ``DataLoader`` call of pyTorch which you will use to create the data later on. The input to ``collate_fn`` is a list of tensors with the size of batch_size, and the ``collate_fn`` function packs them into a mini-batch. Pay attention here and make sure that ``collate_fn`` is declared as a top level definition. This ensures that the function is available in each worker. This is the reason why you need to define this custom function first before you call DataLoader().\n","\n","The text entries in the original data batch input are packed into a list and concatenated as a single tensor as the input of ``EmbeddingBag``. The offsets is a tensor of delimiters to represent the beginning index of the individual sequence in the text tensor. Label is a tensor saving the labels of individual text entries.\n","\n","Finish the function definition below. The function should take batch as an input parameter. Each entry in the batch contains a pair of values of the text and the corresponding label."]},{"cell_type":"code","metadata":{"id":"l6kYMb5L8Gm0","colab_type":"code","colab":{}},"source":["# TODO: Finish the function definition.\n","\n","def generate_batch(raw_batch):\n","    label = torch.tensor([entry[0] for entry in raw_batch])\n","    text = [entry[1] for entry in raw_batch]\n","    offsets = [0] + [len(entry) for entry in text]\n","\n","    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n","    text = torch.cat(text)\n","    \n","    return text, offsets, label"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rJuP_-XI8GnM","colab_type":"text"},"source":["## Define the train function\n","\n","Here, you need to define a function which you will use later on in the project to train your model. This is very similar to the training steps that you have encountered before in previous coding assignment(s). The outline of the function is something like this -\n","\n","* load the data as batches\n","* iterate over the batches\n","* find the model output for a forward pass\n","* calculate the loss\n","* perform backpropagation on the loss (optimize it)\n","* find the training accuracy\n","\n","In addition to this, you also need to find the total loss and total training accuracy values. Also, you need to return the average values of the total loss and total accuracy."]},{"cell_type":"code","metadata":{"id":"m18xHH1w8GnP","colab_type":"code","colab":{}},"source":["def train(train_data):\n","\n","    # Initial values of training loss and training accuracy\n","    \n","    train_loss = 0\n","    train_acc = 0\n","\n","    # TODO: Use the PyTorch DataLoader class to load the data \n","    # into shuffled batches of appropriate sizes into the variable 'data'.\n","    # Remember, this is the place where you need to generate batches.\n","    data = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=generate_batch, shuffle=True)\n","    \n","    \n","    for i, (text, offsets, cls) in tqdm(enumerate(data)):\n","        \n","        # TODO: What do you need to do in order to perform backprop on the optimizer?\n","        optimizer.zero_grad()\n","        \n","        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n","\n","        # TODO: Store the output of the model in variable 'output'\n","        output = model(text, offsets)\n","        \n","       # print(f\"Output: {output}\")\n","        #print(f\"Text: {text}\")\n","        #print(f\"Offsets: {offsets}\")\n","        #print(f\"Cls: {cls}\")\n","        \n","        # TODO: Define the 'loss' variable (with respect to 'output' and 'cls').\n","        loss = criterion(output, cls)\n","        # Also calculate the total loss in variable 'train_loss'\n","        train_loss += loss\n","        \n","        # TODO: Perform the backward propagation on 'loss' and \n","        # optimize it through the 'optimizer' step\n","        loss.backward()\n","        optimizer.step()\n","        \n","        \n","        # TODO: Calculate and store the total training accuracy\n","        # in the variable 'total_acc'.\n","        # Remember, you need to find the \n","        _, pred_labels = output.max(dim=1)\n","        accuracy = (pred_labels == cls).sum() / float(BATCH_SIZE)\n","        train_acc += accuracy\n","        \n","\n","    # TODO: Adjust the learning rate here using the scheduler step\n","    scheduler.step()\n","    \n","    # TODO: CHANGE THIS\n","    return train_loss / len(data), train_acc / len(data)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_qYCJBiZ8GnY","colab_type":"text"},"source":["## Define the test function\n","\n","Using the framework of the `train()` function in the previous cell, try to figure out the structure of the test function below."]},{"cell_type":"code","metadata":{"id":"xavuY7WK8Gnb","colab_type":"code","colab":{}},"source":["def test(test_data):\n","    \n","    # Initial values of test loss and test accuracy\n","    \n","    loss = 0\n","    acc = 0\n","    \n","    # TODO: Use DataLoader class to load the data\n","    # into non-shuffled batches of appropriate sizes.\n","    # Remember, you need to generate batches here too.\n","    data = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, collate_fn=generate_batch, shuffle=False)\n","    \n","    \n","    for text, offsets, cls in data:\n","        \n","        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n","        \n","        # Hint: There is a 'hidden hint' here. Let's see if you can find it :)\n","        with torch.no_grad():\n","        \n","            \n","            # TODO: Get the model output\n","            output = model(text, offsets)\n","            \n","            \n","            \n","            # TODO: Calculate and add the loss to find total 'loss'\n","            loss += criterion(output, cls)\n","        \n","            \n","            \n","            # TODO: Calculate the accuracy and store it in the 'acc' variable\n","            _, pred_labels = output.max(dim=1)\n","            acc += (pred_labels == cls).sum() / float(BATCH_SIZE)\n","            \n","\n","    return loss / len(data), acc / len(data)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lxIrTmqt8Gnr","colab_type":"text"},"source":["## Split the dataset and run the model\n","\n","The original `AG_NEWS` has no validation dataset. For this reason, you need to split the training dataset into training and validation sets with a proper split ratio. The `random_split()` function in the torch.utils core PyTorch library should be able to help you with this. We have already imported it for you. :)\n","\n","* Consider the initial learning rate as 4.0, number of epochs as 5, training data ratio as 0.9.\n","* You need to define and use a proper loss function\n","* Define an Optimization algorithm (Suggestion: SGD)\n","* Define a scheduler function to adjust the learning rate through epochs (gamma parameter = 0.9).\n","(Hint: Look at the `StepLR` function)\n","* Monitor the loss and accuracy values for both training and validation data sets."]},{"cell_type":"code","metadata":{"id":"BytGrFvv8Gnw","colab_type":"code","outputId":"43712182-ac00-4fda-d32a-8bcd4198868d","executionInfo":{"status":"ok","timestamp":1583238349944,"user_tz":-60,"elapsed":262014,"user":{"displayName":"kai glauber","photoUrl":"","userId":"07279436276219446584"}},"colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["import time\n","from torch.utils.data.dataset import random_split\n","\n","# TODO: Set the number of epochs and the learning rate to \n","# their initial values here\n","\n","# TODO: FIGURE THIS OUT\n","N_EPOCHS = 1\n","LEARNING_RATE = 4.0\n","TRAIN_RATIO = 0.9\n","\n","# TODO: Set the intial validation loss to positive infinity\n","INIT_VAL_LOSS = float('inf')\n","\n","\n","# TODO: Use the appropriate loss function\n","criterion = nn.CrossEntropyLoss()\n","\n","\n","# TODO: Use the appropriate optimization algorithm with parameters (Suggested: SGD)\n","optimizer = torch.optim.Adam(model.parameters())\n","\n","# TODO: Use a scheduler function\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n","\n","\n","\n","# TODO: Split the data into train and validation sets using random_split()\n","size = len(train_dataset)\n","split_size = int(size * TRAIN_RATIO)\n","train_dataset_split, validation_dataset_split = random_split(train_dataset, [split_size, size - split_size])\n","\n","\n","# TODO: Finish the rest of the code below\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    train_loss, train_acc = train(train_dataset_split)\n","    valid_loss, valid_acc = test(validation_dataset_split)\n","\n","    secs = int(time.time() - start_time)\n","    mins = secs / 60\n","    secs = secs % 60\n","\n","    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n","    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n","    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["6750it [03:34, 31.11it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 1  | time in 3 minutes, 34 seconds\n","\tLoss: 0.4414(train)\t|\tAcc: 87.3%(train)\n","\tLoss: 0.2528(valid)\t|\tAcc: 92.0%(valid)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LFeOKuiT8Gn7","colab_type":"text"},"source":["## Let's  check the test loss and test accuracy\n","\n","So you have trained your model and seen how well it performs on the training and validation datasets. Now, you need to check your model's performance against the test dataset. Using the test dataset as input, report the test loss and test accuracy scores of your model."]},{"cell_type":"code","metadata":{"id":"BsfvdUWF8Gn-","colab_type":"code","outputId":"86f976cd-62ee-47c5-cbd1-45d982c1f0da","executionInfo":{"status":"ok","timestamp":1583238350522,"user_tz":-60,"elapsed":262541,"user":{"displayName":"kai glauber","photoUrl":"","userId":"07279436276219446584"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# TODO: Compete the code below to find \n","# the results (loss and accuracy) on the test data\n","\n","print('Checking the results of test dataset...')\n","test_loss, test_acc = test(test_dataset)\n","print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Checking the results of test dataset...\n","\tLoss: 0.2790(test)\t|\tAcc: 90.8%(test)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JCUmcL818GoN","colab_type":"code","outputId":"7cfa09ff-f399-415f-db40-ccfa38532d75","executionInfo":{"status":"ok","timestamp":1583238350845,"user_tz":-60,"elapsed":262821,"user":{"displayName":"kai glauber","photoUrl":"","userId":"07279436276219446584"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# importing necessary libraries\n","\n","import re\n","from torchtext.data.utils import ngrams_iterator\n","from torchtext.data.utils import get_tokenizer\n","\n","# labels for the AG_NEWS dataset\n","\n","ag_news_label = {1 : \"World\",\n","                 2 : \"Sports\",\n","                 3 : \"Business\",\n","                 4 : \"Sci/Tec\"}\n","\n","def predict(text, model, vocab, ngrams):\n","    tokenizer = get_tokenizer(\"basic_english\")\n","    with torch.no_grad():\n","        text = torch.tensor([vocab[token]\n","                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n","        output = model(text, torch.tensor([0]))\n","        return output.argmax(1).item() + 1\n","\n","ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n","    enduring the season’s worst weather conditions on Sunday at The \\\n","    Open on his way to a closing 75 at Royal Portrush, which \\\n","    considering the wind and the rain was a respectable showing. \\\n","    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n","    was another story. With temperatures in the mid-80s and hardly any \\\n","    wind, the Spaniard was 13 strokes better in a flawless round. \\\n","    Thanks to his best putting performance on the PGA Tour, Rahm \\\n","    finished with an 8-under 62 for a three-stroke lead, which \\\n","    was even more impressive considering he’d never played the \\\n","    front nine at TPC Southwind.\"\n","\n","vocab = train_dataset.get_vocab()\n","model = model.to(\"cpu\")\n","\n","# TODO: Predict the topic of the above given random text (use bigrams)\n","# Use the proper paramters in the predict() function\n","\n","print(\"This is a '%s' news\" % ag_news_label[predict(ex_text_str, model, vocab, NGRAMS)])\n","\n","# If you have done everything correctly in this task,\n","# then the output of this cell should be - \"This is a 'Sports' news\"."],"execution_count":10,"outputs":[{"output_type":"stream","text":["This is a 'Sports' news\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gaEnt2JI8Goe","colab_type":"text"},"source":["# Congratulations! You just designed your first neural classifier!\n","\n","And probably you have achieved a good accuracy score too. Great job!\n","\n","## Question 2:\n","You just tested your model with a new sample text. Try to feed some more random examples of similar text (which you think are related to at least one of the four topics _\"World\", \"Sports\", \"Business\", \"Sci/Tec\"_ of our problem) to the model and see how your model reacts. Give at least 3 such examples (You are free to include more examples if you wish to).\n","\n","## Answer 2:\n","\n"]},{"cell_type":"code","metadata":{"id":"esXqwYC7AsK4","colab_type":"code","outputId":"2ee1d29f-c2ca-42bf-97f2-25a383e653dc","executionInfo":{"status":"ok","timestamp":1583238350846,"user_tz":-60,"elapsed":262796,"user":{"displayName":"kai glauber","photoUrl":"","userId":"07279436276219446584"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["#World article\n","article_1 = 'China is still seeing the huge majority of confirmed cases, but they are beginning to climb in other countries that are thousands of miles away.Iran and Italy have both seen surging numbers of infected patients.Irans official death toll stands at 34, but sources within the countrys healthcare system tell BBC Persian that the count could be as high as 210. Iran has denied that it is withholding information about the number of dead and infected.Seventeen people have died in Italy.There have also been 13 deaths in South Korea and three deaths in Japan.'\n","#Sports article\n","article_2 = 'Juventus lost 1-0 at Lyon in a Champions League last 16 first leg tie on Wednesday night.The goal came from Lucas Tousart.The 22-year-old French midfielder slotted home in the 31st minute.Juve have won the scudetto eight times in a row but have not bagged Europes premier silverware since 1996, when they won their second Champions League title since the Haysel-disaster-marred win in 1985. To help them win another European crown, they signed Cristiano Ronaldo from Real Madrid the summer before last, but again had no joy.They have again prioritised Europe this year even though they appear to be heading for a possible ninth straight Serie A title despite strong challenges from Lazio and Inter.'\n","#Business article\n","article_3 = 'Dozens of big companies have warned, for example, that the coronavirus will hit their share price.Trouble over an extended period could have an effect on the amount of work available, says Moira ONeill from Interactive Investor. Stock market falls also affect business confidence and the ability of companies to raise money, she says. So if this short-term blip becomes something more pronounced, there could be an impact for the wider economy and maybe your job.'\n","\n","# TODO: Predict the topic of the above given random text (use bigrams)\n","# Use the proper paramters in the predict() function\n","\n","print(\"This is a '%s' news\" % ag_news_label[predict(article_1, model, vocab, NGRAMS)])\n","print(\"This is a '%s' news\" % ag_news_label[predict(article_2, model, vocab, NGRAMS)])\n","print(\"This is a '%s' news\" % ag_news_label[predict(article_3, model, vocab, NGRAMS)])\n","\n","\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["This is a 'World' news\n","This is a 'Sports' news\n","This is a 'Business' news\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"W6UVITwQ8Gop","colab_type":"text"},"source":["## Question 3:\n","Okay, probably the model still works great with the examples you fed to it in the previous question. How about a twist in the plot? Let's feed it some more random text data from completely different genres/topics (not belonging to the 4 topics which we talk about the in the first question). How does your model react now? Give at least 3 such examples (You are free to include more examples if you wish to).\n","\n","Of course the predictions will be limited to the four class labels that your model is trained on. Can you somehow justify the labels that your model predicted now for the given text inputs?\n","\n","## Answer 3:\n"]},{"cell_type":"markdown","metadata":{"id":"32aEgbfNRTNv","colab_type":"text"},"source":["The article about tiger has been categorizd as World. This could be possibly \n","due to the presence of words like 'regions', or the areas which the tiger inhabitates. Also the artice has mentioned words like 'rivers', 'China', 'Indian Subcontinent' which makes it more close to being a World Article.\n","\n","The articel about changing tyres got categorized into business. This maybe because the words in the article are more similar(latent or semantic similarity) to words appearing in the business article. \n","\n","The article about cooking pasta has terms like 'boiling water', 'tongs', 'add salt' which resemble a scientifc experiment (chemistry lab stuff) and hence it is more close to science and technology.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"WWIw7aqMFxwZ","colab_type":"code","outputId":"c279bac0-75b4-4146-f4c9-e9cb9c6b2994","executionInfo":{"status":"ok","timestamp":1583238350847,"user_tz":-60,"elapsed":262777,"user":{"displayName":"kai glauber","photoUrl":"","userId":"07279436276219446584"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["#Animals\n","article_1 = \"\"\"The tiger (Panthera tigris) is the largest species among the Felidae and classified in the genus Panthera. \n","It is most recognisable for its dark vertical stripes on orangish-brown fur with a lighter underside. \n","It is an apex predator, primarily preying on ungulates such as deer and wild boar. \n","It is territorial and generally a solitary but social predator, requiring large contiguous areas of habitat, \n","which support its requirements for prey and rearing of its offspring. \n","Tiger cubs stay with their mother for about two years, before they become independent and leave their mothers home range to establish their own.\n","The tiger once ranged widely from the Eastern Anatolia Region in the west to the Amur River basin, and in the south from the foothills of the Himalayas \n","to Bali in the Sunda islands. \n","Since the early 20th century, tiger populations have lost at least 93% of their historic range and have been extirpated in Western and Central Asia, \n","from the islands of Java and Bali, and in large areas of Southeast and South Asia and China. \n","Todays tiger range is fragmented, stretching from Siberian temperate forests to subtropical and tropical forests on the Indian subcontinent and Sumatra.\"\"\"\n","#How to change car tyres\n","article_2 = \"\"\"As soon as you realize you have a flat tire, do not abruptly brake or turn.  \n","Slowly reduce speed and scan your surroundings for a level, straight stretch of road with a wide shoulder. \n","An empty parking lot would be an ideal place. Level ground is good because it will prevent your vehicle from rolling.\n"," Also, straight stretches of road are better than curves because oncoming traffic is more likely to see you.\"\"\"\n","#Cooking Pasta\n","article_3 = \"\"\"Boil water in a large pot\n","To make sure pasta doesn’t stick together, use at least 4 quarts of water for every pound of noodles.\n","Salt the water with at least a tablespoon—more is fine\n","The salty water adds flavor to the pasta.\n","Add pasta\n","Pour pasta into boiling water. Don’t break the pasta; it will soften up within 30 seconds and fit into the pot.\n","Stir the pasta\n","As the pasta starts to cook, stir it well with the tongs so the noodles don’t stick to each other (or the pot).\n","Test the pasta by tasting it\n","Follow the cooking time on the package, but always taste pasta before draining to make sure the texture is right. \n","Pasta cooked properly should be little chewy.\"\"\"\n","\n","# TODO: Predict the topic of the above given random text (use bigrams)\n","# Use the proper paramters in the predict() function\n","\n","print(\"This is a '%s' news\" % ag_news_label[predict(article_1, model, vocab, NGRAMS)])\n","print(\"This is a '%s' news\" % ag_news_label[predict(article_2, model, vocab, NGRAMS)])\n","print(\"This is a '%s' news\" % ag_news_label[predict(article_3, model, vocab, NGRAMS)])\n","\n","\n","# If you have done everything correctly in this task,\n","# then the output of this cell should be - \"This is a 'Sports' news\"."],"execution_count":12,"outputs":[{"output_type":"stream","text":["This is a 'Business' news\n","This is a 'Business' news\n","This is a 'Business' news\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-tELgFFh8Got","colab_type":"text"},"source":["## Question 4:\n","Your model probably has achieved a good accuracy score. However, there may be lots of things that you could still try to do to improve your classifier model. Can you try to list down some improvements that you think would be able to improve the above model's performance?\n","\n","_(Hint: Maybe think about alternate architectures, #layers, hyper-paramters, etc..., but try not to come up with too complex stuff! :) )_\n","\n","## Answer 4:"]},{"cell_type":"markdown","metadata":{"id":"mwcN0GJzMVXp","colab_type":"text"},"source":["We can introduce the following:\n","1. Use ADAM as an adaptive learning rate.\n","2. We can increase the number of hidden layers and number of epochs.\n","3. We can increase the batch size so that the estimates of our gradient are more accurate.\n","4. Use dropout to prevent overfitting.\n","5. We can use batch normalization.\n","6. We can introduce nonlinearity such as reLu or its variants like leakyRelu.\n"," "]},{"cell_type":"markdown","metadata":{"id":"QxVggsYadrFJ","colab_type":"text"},"source":["\n","# Important Notes\n","\n","## NOTE 1:\n","If you want, you can try out the models on other datasets too for comparisons. Although this is not mandatory, it would be really interesting to see how your model performs for data from different domains maybe. Note that you may need to tweak the code a little bit when you are considering other datasets and formats. \n","\n","## NOTE 2:\n","Any form of plagiarism is strictly prohibited. If it is found that you have copied sample code from the internet, the entire team will be penalized.\n","\n","## NOTE 3:\n","Often Jupyter Notebooks tend to stop working or crash due to overload of memory (lot of variables, big neural models, memory-intensive training of models, etc...). Moreover, with more number of tasks, the number of variables that you will be using will surely incerase. Therefore, it is recommended that you use separate notebooks for each _Task_ in this project.\n","\n","## NOTE 4:\n","You are expected to write well-documented code, that is, with proper comments wherever you think is needed. Make sure you write a comprehensive report for the entire project consisting of data analysis, your model architecture, methods used, discussing and comparing the models against the accuracy and loss metrics, and a final conslusion. If you want to prepare separate reports for each _Task_, you could do this in the Jupyter Notebook itself using $Mardown$ and $\\LaTeX$ code if needed. If you want to submit a single report for the entire project, you could submit a PDF file in that case (Word or $\\LaTeX$).\n","\n","All the very best for project 2. Wishing you happy holidays and a very happy new year in advance! :)"]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"colab":{"name":"Project_2_(Task 3).ipynb","provenance":[{"file_id":"1VIv8rH75pMC1Olmqhc-CzkxCGjWdxx_K","timestamp":1582760428247},{"file_id":"18V7eNpBLpjn_2Wsr54qX107AJ4e8VXxV","timestamp":1582725586402},{"file_id":"1e6-Ez6iNmf64ZMqBFCteTyJhBR6SiKQm","timestamp":1582574955988}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"mBrdcAK28Gjr","colab_type":"text"},"source":["# Project 3: Text Classification in PyTorch\n","\n","## Instructions\n","\n","* All the tasks that you need to complete in this project are either coding tasks (mentioned inside the code cells of the notebook with `#TODO` notations) or theoretical questions that you need to answer by editing the markdown question cells.\n","* **Please make sure you read the [Notes](#Important-Notes) section carefully before you start the project.**\n","\n","## Introduction\n","This project deals with neural text classification using PyTorch. Text classification is the process of assigning tags or categories to text according to its content. It's one of the fundamental tasks in Natural Language Processing (NLP) with broad applications such as sentiment analysis, topic labeling, spam detection, and intent detection.\n","\n","Text classification algorithms are at the heart of a variety of software systems that process text data at scale. Email software uses text classification to determine whether incoming mail is sent to the inbox or filtered into the spam folder. Discussion forums use text classification to determine whether comments should be flagged as inappropriate.\n","\n","**_Example:_** A simple example of text classification would be Spam Classification. Consider the bunch of emails that you would receive in the your personal inbox if the email service provider did not have a spam filter algorithm. Because of the spam filter, spam emails get redirected to the Spam folder, while you receive only non-spam (\"_ham_\") emails in your inbox.\n","\n","![](http://blog.yhat.com/static/img/spam-filter.png)\n","\n","## Task\n","Here, we want you to focus on a specific type of text classification task, \"Document Classification into Topics\". It can be addressed as classifying text data or even large documents into separate discrete topics/genres of interest.\n","\n","\n","![](https://miro.medium.com/max/700/1*YWEqFeKKKzDiNWy5UfrTsg.png)\n","\n","In this project, you will be working on classifying given text data into discrete topics or genres. You are given a bunch of text data, each of which has a label attached. We ask you to learn why you think the contents of the documents have been given these labels based on their words. You need to create a neural classifier that is trained on this given information. Once you have a trained classifier, it should be able to predict the label for any new document or text data sample that is fed to it. The labels need not have any meaning to us, nor to you necessarily.\n","\n","## Data\n","There are various datasets that we can use for this purpose. This tutorial shows how to use the text classification datasets in the PyTorch library ``torchtext``. There are different datasets in this library like `AG_NEWS`, `SogouNews`, `DBpedia`, and others. This project will deal with training a supervised learning algorithm for classification using one of these datasets. In task 1 of this project, we will work with the `AG_NEWS` dataset.\n","\n","## Load Data\n","\n","A bag of **ngrams** feature is applied to capture some partial information about the local word order. In practice, bi-grams or tri-grams are applied to provide more benefits as word groups than only one word.\n","\n","**Example:**\n","\n","*\"I love Neural Networks\"*\n","* **Bi-grams:** \"I love\", \"love Neural\", \"Neural Networks\"\n","* **Tri-grams:** \"I love Neural\", \"love Neural Networks\"\n","\n","In the code below, we have loaded the `AG_NEWS` dataset from the ``torchtext.datasets.TextClassification`` package with bi-grams feature. The dataset supports the ngrams method. By setting ngrams to 2, the example text in the dataset will be a list of single words plus bi-grams string."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rkcpLeny4BPf"},"source":["\n","# Task 3: Let your creativity flow!\n","\n","As discussed earlier, you are free to come up with anything in task 3. Think and try to model unique (not too complex!) neural architecture on your own. Remember that this model has to be novel as much as possible, so try not to copy other people's existing work. Using the same data, train the new model, and report the accuracy scores. How much better/worse is this model than the previous two models? Why do you think this is better/worse?\n"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"36b54f77-0954-496f-e044-48aa49eac199","executionInfo":{"status":"ok","timestamp":1583236469255,"user_tz":-60,"elapsed":25530,"user":{"displayName":"kai glauber","photoUrl":"","userId":"07279436276219446584"}},"id":"7Wytoc6nzJ6G","colab":{"base_uri":"https://localhost:8080/","height":410}},"source":["\"\"\"\n","Load the AG_NEWS dataset in bi-gram features format.\n","\"\"\"\n","\n","!pip install torchtext==0.4\n","\n","import os\n","import torch\n","import torchtext\n","from torchtext.datasets import text_classification\n","\n","from tqdm import tqdm\n","\n","\n","if not os.path.isdir('./.data'):\n","    os.mkdir('./.data')\n","\n","train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n","    root='./.data', ngrams=1, vocab=None, include_unk=True)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting torchtext==0.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/929d6bd236a4fb5c435982a7eb9730b78dcd8659acf328fd2ef9de85f483/torchtext-0.4.0-py3-none-any.whl (53kB)\n","\r\u001b[K     |██████▏                         | 10kB 26.3MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 20kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 30kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 40kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 51kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 5.1MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (1.12.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (1.17.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (2.21.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (4.28.1)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (2019.11.28)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (3.0.4)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (1.24.3)\n","Installing collected packages: torchtext\n","  Found existing installation: torchtext 0.3.1\n","    Uninstalling torchtext-0.3.1:\n","      Successfully uninstalled torchtext-0.3.1\n","Successfully installed torchtext-0.4.0\n"],"name":"stdout"},{"output_type":"stream","text":["ag_news_csv.tar.gz: 11.8MB [00:00, 42.8MB/s]\n","120000lines [00:04, 29337.61lines/s]\n","120000lines [00:06, 17459.86lines/s]\n","7600lines [00:00, 18121.97lines/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"a6ba8d93-24e9-4719-bf9d-543d86476e0c","executionInfo":{"status":"ok","timestamp":1583236469257,"user_tz":-60,"elapsed":25507,"user":{"displayName":"kai glauber","photoUrl":"","userId":"07279436276219446584"}},"id":"NkmahdyW0jX5","colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["def getVocabSize(dataset):\n","    return len(dataset.get_vocab())\n","\n","def getClasses(dataset):\n","    return dataset.get_labels()\n","  \n","VOCAB_SIZE = getVocabSize(test_dataset)\n","CLASSES = getClasses(test_dataset)\n","NUM_CLASSES = len(CLASSES)\n","\n","print(f\"VOCAB_SIZE: {VOCAB_SIZE}\")\n","print(f\"NUM_CLASSES: {NUM_CLASSES}\")\n","print(f\"CLASSES: {CLASSES}\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["VOCAB_SIZE: 95812\n","NUM_CLASSES: 4\n","CLASSES: {0, 1, 2, 3}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0sJj7v-RyFY9","colab_type":"text"},"source":["# Hyperparams"]},{"cell_type":"code","metadata":{"id":"Uq2gAAdGs5LA","colab_type":"code","colab":{}},"source":["EMBED_DIM = 100\n","BATCH_SIZE = 1\n","FILTER_DIM = 100\n","NGA = 2\n","NGB = 3 \n","NGC = 4 "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lo29NjymDsh1","colab_type":"text"},"source":["# The model"]},{"cell_type":"code","metadata":{"id":"QHn0zJWGfWJk","colab_type":"code","outputId":"69fe38bd-606b-4eb4-80c9-60aa92a4d943","executionInfo":{"status":"ok","timestamp":1583236479027,"user_tz":-60,"elapsed":35249,"user":{"displayName":"kai glauber","photoUrl":"","userId":"07279436276219446584"}},"colab":{"base_uri":"https://localhost:8080/","height":141}},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class CreativeNN(nn.Module):\n","\n","    def __init__(self, vocab_size, embedding_dim, fil_dim, nga, ngb, ngc, num_classes):\n","        \"\"\"\n","        Initialize the model by setting up the layers.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.num_classes = num_classes\n","        \n","        # Possibly interchange this layer with glove pre trained embeddings\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","\n","        self.cnn1 = nn.Conv2d(1, fil_dim, (nga, embedding_dim))\n","\n","        self.cnn2 = nn.Conv2d(1, fil_dim, (ngb, embedding_dim))\n","        \n","        self.cnn3 = nn.Conv2d(1, fil_dim, (ngc, embedding_dim))\n","\n","        self.linear = nn.Linear(3 * fil_dim, num_classes)\n","\n","        \n","\n","    def forward(self, input):\n","      x = self.embedding(input)\n","\n","      # print(f\"Embedding shape {x.shape}\")\n","\n","      x = x[:, None, :, :]\n","      # print(f\"Embedding postprocessing shape: {x.shape}\")\n","\n","      # =========================================\n","\n","      x1 = F.relu(self.cnn1(x))\n","      x1 = torch.squeeze(x1, 3)\n","      # print(f\"X1 shape: {x1.shape}\")\n","      x1 = F.max_pool1d(x1, x1.shape[2])\n","      # print(f\"X1 shape: {x1.shape}\")\n","      x1 = torch.squeeze(x1, 2)\n","      x1 = torch.squeeze(x1, 0)\n","      # print(f\"X1 shape: {x1.shape}\")\n","\n","      # =========================================\n","\n","      x2 = F.relu(self.cnn2(x))\n","      x2 = torch.squeeze(x2, 3)\n","      # print(f\"X2 shape: {x2.shape}\")\n","      x2 = F.max_pool1d(x2, x2.shape[2])\n","      # print(f\"X2 shape: {x2.shape}\")\n","      x2 = torch.squeeze(x2, 2)\n","      x2 = torch.squeeze(x2, 0)\n","      # print(f\"X2 shape: {x2.shape}\")\n","      \n","      # =========================================\n","\n","      x3 = F.relu(self.cnn3(x))\n","      x3 = torch.squeeze(x3, 3)\n","      # print(f\"X3 shape: {x3.shape}\")\n","      x3 = F.max_pool1d(x3, x3.shape[2])\n","      # print(f\"X3 shape: {x3.shape}\")\n","      x3 = torch.squeeze(x3, 2)\n","      x3 = torch.squeeze(x3, 0)\n","      # print(f\"X3 shape: {x3.shape}\")\n","     \n","      # =========================================\n","\n","      x = torch.cat((x1, x2, x3)).to(device)\n","      # print(\"===========\")\n","      # print(f\"FEATURE: {x.shape}\")\n","      # print(\"===========\")\n","\n","\n","      x = self.linear(x) \n","      x = x[None, :]\n","\n","      return x\n","\n","model = CreativeNN(VOCAB_SIZE, EMBED_DIM, FILTER_DIM, NGA, NGB, NGC, NUM_CLASSES).to(device)\n","print(model)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CreativeNN(\n","  (embedding): Embedding(95812, 100)\n","  (cnn1): Conv2d(1, 100, kernel_size=(2, 100), stride=(1, 1))\n","  (cnn2): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n","  (cnn3): Conv2d(1, 100, kernel_size=(4, 100), stride=(1, 1))\n","  (linear): Linear(in_features=300, out_features=4, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kQpqEIGRECkw","colab_type":"text"},"source":["# The training"]},{"cell_type":"code","metadata":{"id":"LAYWU4LDEFIq","colab_type":"code","colab":{}},"source":["def train(train_data):\n","\n","    # Initial values of training loss and training accuracy\n","    \n","    train_loss = 0\n","    train_acc = 0\n","\n","    # TODO: Use the PyTorch DataLoader class to load the data \n","    # into shuffled batches of appropriate sizes into the variable 'data'.\n","    # Remember, this is the place where you need to generate batches.\n","    data = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n","    \n","    \n","    for i, (cls, text) in tqdm(enumerate(data)):\n","        \n","        # TODO: What do you need to do in order to perform backprop on the optimizer?\n","        optimizer.zero_grad()\n","        \n","        cls, text = cls.to(device), text.to(device)\n","\n","        # TODO: Store the output of the model in variable 'output'\n","        output = model(text)\n","\n","        # print(f\"Output: {output}\")\n","        # print(f\"Cls: {cls}\")\n","\n","        # TODO: Define the 'loss' variable (with respect to 'output' and 'cls').\n","        loss = criterion(output, cls)\n","        # Also calculate the total loss in variable 'train_loss'\n","        train_loss += loss\n","        \n","        # TODO: Perform the backward propagation on 'loss' and \n","        # optimize it through the 'optimizer' step\n","        loss.backward()\n","        optimizer.step()\n","        \n","        \n","        # TODO: Calculate and store the total training accuracy\n","        # in the variable 'total_acc'.\n","        # Remember, you need to find the \n","        _, pred_labels = output.max(dim=1)\n","        \n","        accuracy = (pred_labels == cls).sum() / float(BATCH_SIZE)\n","        train_acc += accuracy\n","        \n","\n","    # TODO: Adjust the learning rate here using the scheduler step\n","    scheduler.step()\n","    \n","    # TODO: CHANGE THIS\n","    return train_loss / len(data), train_acc / len(data)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pw-M_3DbQ9LP","colab_type":"text"},"source":["# The testing"]},{"cell_type":"code","metadata":{"id":"ZIqQ4LodRBP-","colab_type":"code","colab":{}},"source":["def test(test_data):\n","    \n","    # Initial values of test loss and test accuracy\n","    \n","    loss = 0\n","    acc = 0\n","    \n","    # TODO: Use DataLoader class to load the data\n","    # into non-shuffled batches of appropriate sizes.\n","    # Remember, you need to generate batches here too.\n","    data = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n","    \n","    \n","    for cls, text in data:\n","        \n","        cls, text = cls.to(device), text.to(device)\n","        \n","        # Hint: There is a 'hidden hint' here. Let's see if you can find it :)\n","        with torch.no_grad():\n","        \n","            \n","            # TODO: Get the model output\n","            output = model(text)\n","            \n","            \n","            # TODO: Calculate and add the loss to find total 'loss'\n","            loss += criterion(output, cls)\n","            \n","            \n","            # TODO: Calculate the accuracy and store it in the 'acc' variable\n","            _, pred_labels = output.max(dim=1)\n","            acc += (pred_labels == cls).sum() / float(BATCH_SIZE)\n","            \n","\n","    return loss / len(data), acc / len(data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"g1C26WJu33iw","colab":{}},"source":["import time\n","from torch.utils.data.dataset import random_split\n","\n","# TODO: Set the number of epochs and the learning rate to \n","# their initial values here\n","\n","# TODO: FIGURE THIS OUT\n","N_EPOCHS = 3 \n","TRAIN_RATIO = 0.9\n","\n","# TODO: Set the intial validation loss to positive infinity\n","INIT_VAL_LOSS = float('inf')\n","\n","\n","# TODO: Use the appropriate loss function\n","criterion = nn.CrossEntropyLoss()\n","\n","\n","# TODO: Use the appropriate optimization algorithm with parameters (Suggested: SGD)\n","# optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n","optimizer = torch.optim.Adam(model.parameters())\n","\n","\n","# TODO: Use a scheduler function\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n","\n","\n","\n","# TODO: Split the data into train and validation sets using random_split()\n","size = len(train_dataset)\n","split_size = int(size * TRAIN_RATIO)\n","train_dataset_split, validation_dataset_split = random_split(train_dataset, [split_size, size - split_size])\n","\n","# TODO: Finish the rest of the code below\n","\n","DEBUG = True\n","\n","if DEBUG:\n","  pass\n","\n","else:\n","    for epoch in range(N_EPOCHS):\n","\n","        start_time = time.time()\n","        train_loss, train_acc = train(train_dataset_split)\n","        valid_loss, valid_acc = test(validation_dataset_split)\n","    \n","        secs = int(time.time() - start_time)\n","        mins = secs / 60\n","        secs = secs % 60\n","    \n","        print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n","        print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n","        print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UsyPgQxxNdUl","colab_type":"text"},"source":["# Evaluation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UWtL3b7dsHG9","colab":{}},"source":["def trainAndSafe(embedding_dim, filter_dim, ng_list):\n","\n","  nga, ngb, ngc = ng_list\n","  filename = f\"/content/gdrive/My Drive/task3_ed-fd-ng_{embedding_dim, filter_dim, nga, ngb, ngc}\"\n","  model = CreativeNN(VOCAB_SIZE, embedding_dim, filter_dim, nga, ngb, ngc, NUM_CLASSES).to(device)\n","\n","  INIT_VAL_LOSS = float('inf')\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = torch.optim.Adam(model.parameters())\n","  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n","  \n","  size = len(train_dataset)\n","  split_size = int(size * TRAIN_RATIO)\n","  train_dataset_split, validation_dataset_split = random_split(train_dataset, [split_size, size - split_size])\n","  \n","  for epoch in range(N_EPOCHS):\n","\n","        start_time = time.time()\n","        train_loss, train_acc = train(train_dataset_split)\n","        valid_loss, valid_acc = test(validation_dataset_split)\n","        test_loss, test_acc = test(test_dataset)\n","    \n","        secs = int(time.time() - start_time)\n","        mins = secs / 60\n","        secs = secs % 60\n","\n","       \n","        data = {\n","            'epoch': epoch,\n","            'mins': mins,\n","            'secs': secs,\n","            'model_state_dict': model.state_dict(),\n","            'train_loss': train_loss,\n","            'train_acc': train_acc,\n","            'valid_loss': valid_loss,\n","            'valid_acc': valid_acc,\n","            'test_loss': test_loss,\n","            'test_acc': test_acc\n","        }\n","\n","        torch.save(data, f\"{filename}_{epoch}\")\n","\n","        print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n","        print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n","        print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')\n","        print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')\n","\n","def loadModel(embedding_dim, filter_dim, ng_list, epoch):\n","  nga, ngb, ngc = ng_list\n","\n","  filename = f\"/content/gdrive/My Drive/task3_ed-fd-ng_{embedding_dim, filter_dim, nga, ngb, ngc}\"\n","  data = torch.load(f\"{filename}_{epoch}\")\n","\n","  return data\n","\n","\n","def d2s(data, params, epoch=-1):\n","    if epoch == -1 or epoch-1 == data['epoch']:\n","      print(f\"Task3: {params}\")\n","      print('Epoch: %d' %(data[\"epoch\"] + 1), \" | time in %d minutes, %d seconds\" %(data[\"mins\"], data[\"secs\"]))\n","      print(f'\\tLoss: {data[\"train_loss\"]:.4f}(train)\\t|\\tAcc: {data[\"train_acc\"] * 100:.1f}%(train)')\n","      print(f'\\tLoss: {data[\"valid_loss\"]:.4f}(valid)\\t|\\tAcc: {data[\"valid_acc\"] * 100:.1f}%(valid)')\n","\n","\n","def bestAcc(data, epoch, previousBestAcc, params):\n","  if data['epoch'] == epoch and data['valid_acc'].item() > previousBestAcc[0]:\n","    return [data['train_acc'].item(), params]\n","  return previousBestAcc\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u--roR5cjM9S","colab_type":"text"},"source":["# Do your experiments here"]},{"cell_type":"code","metadata":{"id":"4KmykSBnkiSM","colab_type":"code","outputId":"57b79033-a430-4872-90ac-968b8b3e9aff","executionInfo":{"status":"ok","timestamp":1583236507889,"user_tz":-60,"elapsed":64065,"user":{"displayName":"kai glauber","photoUrl":"","userId":"07279436276219446584"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"neEyV-rzUcvE","colab_type":"code","colab":{}},"source":["TRAIN = False\n","\n","if TRAIN:\n","  trainAndSafe(32, 32, [2, 3, 4])\n","  trainAndSafe(32, 64, [2, 3, 4])\n","  trainAndSafe(64, 32, [2, 3, 4])\n","  trainAndSafe(64, 64, [2, 3, 4])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OO3D_oZBrULR","colab":{}},"source":["hyperparams = [[32, 32, [2, 3, 4]],\n","               [32, 64, [2, 3, 4]],\n","               [64, 32, [2, 3, 4]],\n","               [64, 64, [2, 3, 4]]]\n","\n","\n","bestAccForEpoch = [[0]] * N_EPOCHS\n","\n","for hps in hyperparams:\n","  for epoch in range(N_EPOCHS):\n","    previousBest = bestAccForEpoch[epoch]\n","    try:\n","      data = loadModel(*hps, epoch)\n","      bestAccForEpoch[epoch] = bestAcc(data, epoch, previousBest, hps)\n","      d2s(data, hps, N_EPOCHS)\n","    except:\n","      pass"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aTmCck7D4_NE","colab_type":"code","outputId":"c74861ac-c801-4827-8a27-f74543905a70","executionInfo":{"status":"ok","timestamp":1583239269925,"user_tz":-60,"elapsed":1369230,"user":{"displayName":"kai glauber","photoUrl":"","userId":"07279436276219446584"}},"colab":{"base_uri":"https://localhost:8080/","height":283}},"source":["\n","trainAndSafe(64, 64, [2, 3, 4])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["108000it [07:17, 246.88it/s]\n","21it [00:00, 201.87it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 1  | time in 7 minutes, 34 seconds\n","\tLoss: 0.3134(train)\t|\tAcc: 91.4%(train)\n","\tLoss: 0.2919(valid)\t|\tAcc: 91.9%(valid)\n","\tLoss: 0.3872(test)\t|\tAcc: 89.9%(test)\n"],"name":"stdout"},{"output_type":"stream","text":["108000it [07:16, 247.41it/s]\n","21it [00:00, 209.98it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 2  | time in 7 minutes, 33 seconds\n","\tLoss: 0.2466(train)\t|\tAcc: 93.2%(train)\n","\tLoss: 0.3735(valid)\t|\tAcc: 90.5%(valid)\n","\tLoss: 0.5121(test)\t|\tAcc: 88.3%(test)\n"],"name":"stdout"},{"output_type":"stream","text":["108000it [07:17, 208.68it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 3  | time in 7 minutes, 34 seconds\n","\tLoss: 0.1873(train)\t|\tAcc: 94.9%(train)\n","\tLoss: 0.4036(valid)\t|\tAcc: 91.5%(valid)\n","\tLoss: 0.5482(test)\t|\tAcc: 90.1%(test)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7LC6lOXx5sre","colab_type":"text"},"source":["| embedding_dim \t| hidden_size \t| num_layers \t| train acc \t| val acc \t|\n","|---------------\t|-------------\t|------------\t|-----------\t|---------\t|\n","| 32            \t| 32          \t| [2, 3, 4]  \t| 91.5%     \t| 90.3%   \t|\n","| 32            \t| 64          \t| [2, 3, 4]  \t| 91.8%     \t| 88.5%   \t|\n","| 64            \t| 32          \t| [2, 3, 4]  \t| 91.7%     \t| 89.6%   \t|\n","| 64            \t| 64          \t| [2, 3, 4]  \t| 91.6%     \t| 90.4%   \t|"]},{"cell_type":"markdown","metadata":{"id":"nspXlugEbDxE","colab_type":"text"},"source":["# Model Architecture:\n","1. Single Convolutionl layer using 3 convolutions with kernels of following sizes:\n","\n","    *   [2 x embedding_dimension]\n","    *   [3 x embedding_dimension]\n","    *   [4 x embedding_dimension]\n","  \n","  The input to the convolution layer is the word embeddings of a single article derived from the embedding layer.\n","2.   Embedding dimension is a hyperparameter. 32 and 64 are the candidates for the embedding_dimension and we select the one using validation set approach.\n","3. The convolution is followed by reLU function. \n","4. Since the width of the kernels equals the embedding_dimension, the kernels can only stride downwards i.e. along the height of the input. The height of the input is the number of words in the input(passage). We use stride of **1** for each kernel.\n","5. As a result, the output of convolution is a vector rather than a matrix (one dimensional convolution).\n","6. The number of kernels used of each type is a hyperparameter. We call this hyperparameter as '*filter_dim*'. The candidates are 32 and 64 and we select the one using validation set.\n","7. After using the reLU function, we use Max Pooling. The Max Pooling simply selects the highest value from each vector. The outputs of Max Pooling are combined into a new vector which is fed as an input to feed forward neural network.\n","8. The feedforward network has 1 input layer and 1 output layer. There is no hidden layer.We use cross entropy function to calculate the loss.\n","9. To optimize our network, we are using **ADAM** to adapt the learning rate.\n","10. We are using a batch size of one. This is because different articles will have different word lengths and convolution layer expects a fixed input size for each articles in the batch.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"OsmbdAHo7I3Y","colab_type":"text"},"source":["# Conclusion\n","\n","The model yields comparable results to the first two. Although we did not make use of batching in this task (to avoid introducing noise as in part 2) training the model does not take much time when compared to lstm. When not using batching in lstm training the model for a given set of hyperparameters took around 1.5 hours as opposed to the cnn approach which only takes 15 minutes for one epoch. While it did not perform better than the first or second task we are confident that by tweaking the hyperparameters it could surpass both the previous networks. Also introducing a batch size greater than 1 would speed up the process as well. The reason why we think it might be able to outperform the first one is that it also gets a notion of bigrams and trigrams and even four grams by providing it with kernels of theses sizes. The convolution thus learns features comparable to the feateres learnt in the first task. All in all we have shown that all three approaches are working very well for categorizing articles. "]}]}
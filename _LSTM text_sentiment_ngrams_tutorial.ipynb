{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":" LSTM text_sentiment_ngrams_tutorial.ipynb","provenance":[{"file_id":"1lbx4CxFlCn2tflqJZ0-gAGQ8598-WFmj","timestamp":1587044025328},{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/text_sentiment_ngrams_tutorial.ipynb","timestamp":1586129626960}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"4iaf64--1GVp","colab_type":"code","outputId":"d3c9fd0c-75fa-4eab-c0e5-f07bb002e0c2","executionInfo":{"status":"ok","timestamp":1587053770729,"user_tz":-120,"elapsed":7249,"user":{"displayName":"Mehran Jeelani","photoUrl":"https://lh3.googleusercontent.com/-PDuprpGjLik/AAAAAAAAAAI/AAAAAAAACIc/wUSx8bhwcxA/s64/photo.jpg","userId":"00272410087232989111"}},"colab":{"base_uri":"https://localhost:8080/","height":208}},"source":["!pip install torch==1.4.0\n","!pip install torchtext==0.4\n","%matplotlib inline"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch==1.4.0 in /usr/local/lib/python3.6/dist-packages (1.4.0)\n","Requirement already satisfied: torchtext==0.4 in /usr/local/lib/python3.6/dist-packages (0.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (1.18.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (1.12.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (1.4.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (2.21.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (4.38.0)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (2020.4.5.1)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (3.0.4)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5jBLHs8r1GVs","colab_type":"text"},"source":["\n","Text Classification Tutorial\n","============================\n","\n","This tutorial shows how to use the text classification datasets,\n","including\n","\n","::\n","\n","   - AG_NEWS,\n","   - SogouNews, \n","   - DBpedia, \n","   - YelpReviewPolarity,\n","   - YelpReviewFull, \n","   - YahooAnswers, \n","   - AmazonReviewPolarity,\n","   - AmazonReviewFull\n","\n","This example shows the application of ``TextClassification`` Dataset for\n","supervised learning analysis.\n","\n","Load data with ngrams\n","---------------------\n","\n","A bag of ngrams feature is applied to capture some partial information\n","about the local word order. In practice, bi-gram or tri-gram are applied\n","to provide more benefits as word groups than only one word. An example:\n","\n","::\n","\n","   \"load data with ngrams\"\n","   Bi-grams results: \"load data\", \"data with\", \"with ngrams\"\n","   Tri-grams results: \"load data with\", \"data with ngrams\"\n","\n","``TextClassification`` Dataset supports the ngrams method. By setting\n","ngrams to 2, the example text in the dataset will be a list of single\n","words plus bi-grams string.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"MV_DpdW01GVs","colab_type":"code","outputId":"4bc5678b-3313-4dc4-d9f7-080c5936a53d","executionInfo":{"status":"ok","timestamp":1587054421104,"user_tz":-120,"elapsed":37800,"user":{"displayName":"Mehran Jeelani","photoUrl":"https://lh3.googleusercontent.com/-PDuprpGjLik/AAAAAAAAAAI/AAAAAAAACIc/wUSx8bhwcxA/s64/photo.jpg","userId":"00272410087232989111"}},"colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["import torch\n","import torchtext\n","from torchtext.datasets import text_classification\n","NGRAMS = 2\n","import os\n","if not os.path.isdir('./.data'):\n","\tos.mkdir('./.data')\n","train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n","    root='./.data', ngrams=NGRAMS, vocab=None)\n","BATCH_SIZE = 16\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print('device is ',device)"],"execution_count":33,"outputs":[{"output_type":"stream","text":["120000lines [00:08, 13951.29lines/s]\n","120000lines [00:20, 5841.56lines/s]\n","7600lines [00:01, 6536.20lines/s]"],"name":"stderr"},{"output_type":"stream","text":["device is  cuda\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"NyZO6TR71GVu","colab_type":"text"},"source":["Define the model\n","----------------\n","\n","The model is composed of the\n","`EmbeddingBag <https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag>`__\n","layer and the linear layer (see the figure below). ``nn.EmbeddingBag``\n","computes the mean value of a “bag” of embeddings. The text entries here\n","have different lengths. ``nn.EmbeddingBag`` requires no padding here\n","since the text lengths are saved in offsets.\n","\n","Additionally, since ``nn.EmbeddingBag`` accumulates the average across\n","the embeddings on the fly, ``nn.EmbeddingBag`` can enhance the\n","performance and memory efficiency to process a sequence of tensors.\n","\n","![](https://github.com/pytorch/tutorials/blob/gh-pages/_static/img/text_sentiment_ngrams_model.png?raw=1)\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"2f7IFssG1GVv","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","class TextSentiment(nn.Module):\n","    def __init__(self, vocab_size, embed_dim,hidden_dim, num_class):\n","        super().__init__()\n","        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n","        self.hidden_dim = hidden_dim\n","        self.lstm = nn.LSTM(embed_dim, hidden_dim)\n","        self.fc = nn.Linear(hidden_dim, num_class)\n","        self.init_weights()\n","        self.hidden = self.init_hidden()\n","        \n","        \n","\n","    def init_weights(self):\n","        initrange = 0.5\n","        self.embedding.weight.data.uniform_(-initrange, initrange)\n","        self.fc.weight.data.uniform_(-initrange, initrange)\n","        self.fc.bias.data.zero_()\n","    def init_hidden(self):\n","         return (torch.zeros(1, 1, self.hidden_dim,device=device),\n","                torch.zeros(1, 1, self.hidden_dim,device=device))\n","        \n","    def forward(self, text, offsets):\n","        embeds = self.embedding(text, offsets)\n","        #print('shape of output ',self.fc(embedded).shape)\n","        lstm_out, self.hidden = self.lstm(\n","            embeds.view(embeds.size()[0], 1, -1), self.hidden)\n","        class_space = self.fc(lstm_out.view(embeds.size()[0], -1))\n","        return class_space"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xdmrsNCu1GVx","colab_type":"text"},"source":["Initiate an instance\n","--------------------\n","\n","The AG_NEWS dataset has four labels and therefore the number of classes\n","is four.\n","\n","::\n","\n","   1 : World\n","   2 : Sports\n","   3 : Business\n","   4 : Sci/Tec\n","\n","The vocab size is equal to the length of vocab (including single word\n","and ngrams). The number of classes is equal to the number of labels,\n","which is four in AG_NEWS case.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"b7ZDiTLx1GVz","colab_type":"code","outputId":"5b91ac1f-f66d-4e6a-d443-c752d38a5ddb","executionInfo":{"status":"ok","timestamp":1587053809219,"user_tz":-120,"elapsed":45701,"user":{"displayName":"Mehran Jeelani","photoUrl":"https://lh3.googleusercontent.com/-PDuprpGjLik/AAAAAAAAAAI/AAAAAAAACIc/wUSx8bhwcxA/s64/photo.jpg","userId":"00272410087232989111"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["VOCAB_SIZE = len(train_dataset.get_vocab())\n","print(VOCAB_SIZE)\n","EMBED_DIM = 32\n","HIDDEN_SIZE = 64\n","NUN_CLASS = len(train_dataset.get_labels())\n","model = TextSentiment(VOCAB_SIZE, EMBED_DIM, HIDDEN_SIZE ,NUN_CLASS).to(device)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["1308844\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZEsO7Tx41GV1","colab_type":"text"},"source":["Functions used to generate batch\n","--------------------------------\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4MM7xXE51GV2","colab_type":"text"},"source":["Since the text entries have different lengths, a custom function\n","generate_batch() is used to generate data batches and offsets. The\n","function is passed to ``collate_fn`` in ``torch.utils.data.DataLoader``.\n","The input to ``collate_fn`` is a list of tensors with the size of\n","batch_size, and the ``collate_fn`` function packs them into a\n","mini-batch. Pay attention here and make sure that ``collate_fn`` is\n","declared as a top level def. This ensures that the function is available\n","in each worker.\n","\n","The text entries in the original data batch input are packed into a list\n","and concatenated as a single tensor as the input of ``nn.EmbeddingBag``.\n","The offsets is a tensor of delimiters to represent the beginning index\n","of the individual sequence in the text tensor. Label is a tensor saving\n","the labels of individual text entries.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"QMRPChyE1GV2","colab_type":"code","colab":{}},"source":["def generate_batch(batch):\n","    #print('first text of batch is ',batch[0][1][0])\n","    label = torch.tensor([entry[0] for entry in batch])\n","    text = [entry[1] for entry in batch]\n","\n","    offsets = [0] + [len(entry) for entry in text]\n","    # torch.Tensor.cumsum returns the cumulative sum\n","    # of elements in the dimension dim.\n","    #print(offsets[:-1])\n","    \n","    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n","    text = torch.cat(text)\n","    return text, offsets, label\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W0EfqOo11GV5","colab_type":"text"},"source":["Define functions to train the model and evaluate results.\n","---------------------------------------------------------\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"h-7I3joT1GV5","colab_type":"text"},"source":["`torch.utils.data.DataLoader <https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader>`__\n","is recommended for PyTorch users, and it makes data loading in parallel\n","easily (a tutorial is\n","`here <https://pytorch.org/tutorials/beginner/data_loading_tutorial.html>`__).\n","We use ``DataLoader`` here to load AG_NEWS datasets and send it to the\n","model for training/validation.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"-pdnHNZd1GV5","colab_type":"code","colab":{}},"source":["from torch.utils.data import DataLoader\n","\n","def train_func(sub_train_):\n","\n","    # Train the model\n","    train_loss = 0\n","    train_acc = 0\n","    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n","                      collate_fn=generate_batch)\n","    print('data type:',type(data))\n","    for i, (text, offsets, cls) in enumerate(data):\n","        optimizer.zero_grad()\n","        model.hidden = model.init_hidden()\n","        #model.hidden.to(device)\n","        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n","        output = model(text, offsets)\n","        loss = criterion(output, cls)\n","        train_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","        train_acc += (output.argmax(1) == cls).sum().item()\n","\n","    # Adjust the learning rate\n","    scheduler.step()\n","    \n","    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n","\n","def test(data_):\n","    loss = 0\n","    acc = 0\n","    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n","    for text, offsets, cls in data:\n","        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n","        with torch.no_grad():\n","            output = model(text, offsets)\n","            loss = criterion(output, cls)\n","            loss += loss.item()\n","            acc += (output.argmax(1) == cls).sum().item()\n","\n","    return loss / len(data_), acc / len(data_)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qggTK2V51GV8","colab_type":"text"},"source":["Split the dataset and run the model\n","-----------------------------------\n","\n","Since the original AG_NEWS has no valid dataset, we split the training\n","dataset into train/valid sets with a split ratio of 0.95 (train) and\n","0.05 (valid). Here we use\n","`torch.utils.data.dataset.random_split <https://pytorch.org/docs/stable/data.html?highlight=random_split#torch.utils.data.random_split>`__\n","function in PyTorch core library.\n","\n","`CrossEntropyLoss <https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss>`__\n","criterion combines nn.LogSoftmax() and nn.NLLLoss() in a single class.\n","It is useful when training a classification problem with C classes.\n","`SGD <https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html>`__\n","implements stochastic gradient descent method as optimizer. The initial\n","learning rate is set to 4.0.\n","`StepLR <https://pytorch.org/docs/master/_modules/torch/optim/lr_scheduler.html#StepLR>`__\n","is used here to adjust the learning rate through epochs.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"OVHnSoRq1GV8","colab_type":"code","colab":{}},"source":["import time\n","from torch.utils.data.dataset import random_split\n","N_EPOCHS = 5\n","min_valid_loss = float('inf')\n","\n","criterion = torch.nn.CrossEntropyLoss().to(device)\n","optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n","\n","train_len = int(len(train_dataset) * 0.95)\n","sub_train_, sub_valid_ = \\\n","    random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MWFL2iUS7eSe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":364},"outputId":"07eeea9d-eee9-4830-836d-313a21caad4c","executionInfo":{"status":"ok","timestamp":1587053921353,"user_tz":-120,"elapsed":157789,"user":{"displayName":"Mehran Jeelani","photoUrl":"https://lh3.googleusercontent.com/-PDuprpGjLik/AAAAAAAAAAI/AAAAAAAACIc/wUSx8bhwcxA/s64/photo.jpg","userId":"00272410087232989111"}}},"source":["for epoch in range(5):\n","\n","    start_time = time.time()\n","    train_loss, train_acc = train_func(sub_train_)\n","    valid_loss, valid_acc = test(sub_valid_)\n","\n","    secs = int(time.time() - start_time)\n","    mins = secs / 60\n","    secs = secs % 60\n","\n","    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n","    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n","    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"],"execution_count":27,"outputs":[{"output_type":"stream","text":["data type: <class 'torch.utils.data.dataloader.DataLoader'>\n","Epoch: 1  | time in 0 minutes, 22 seconds\n","\tLoss: 0.0631(train)\t|\tAcc: 52.5%(train)\n","\tLoss: 0.0002(valid)\t|\tAcc: 87.1%(valid)\n","data type: <class 'torch.utils.data.dataloader.DataLoader'>\n","Epoch: 2  | time in 0 minutes, 22 seconds\n","\tLoss: 0.0177(train)\t|\tAcc: 90.3%(train)\n","\tLoss: 0.0002(valid)\t|\tAcc: 90.2%(valid)\n","data type: <class 'torch.utils.data.dataloader.DataLoader'>\n","Epoch: 3  | time in 0 minutes, 22 seconds\n","\tLoss: 0.0111(train)\t|\tAcc: 94.0%(train)\n","\tLoss: 0.0001(valid)\t|\tAcc: 90.8%(valid)\n","data type: <class 'torch.utils.data.dataloader.DataLoader'>\n","Epoch: 4  | time in 0 minutes, 22 seconds\n","\tLoss: 0.0074(train)\t|\tAcc: 96.1%(train)\n","\tLoss: 0.0001(valid)\t|\tAcc: 91.3%(valid)\n","data type: <class 'torch.utils.data.dataloader.DataLoader'>\n","Epoch: 5  | time in 0 minutes, 21 seconds\n","\tLoss: 0.0049(train)\t|\tAcc: 97.4%(train)\n","\tLoss: 0.0001(valid)\t|\tAcc: 91.7%(valid)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3GGBVd0I1GV-","colab_type":"text"},"source":["Running the model on GPU with the following information:\n","\n","Epoch: 1 \\| time in 0 minutes, 11 seconds\n","\n","::\n","\n","       Loss: 0.0263(train)     |       Acc: 84.5%(train)\n","       Loss: 0.0001(valid)     |       Acc: 89.0%(valid)\n","\n","\n","Epoch: 2 \\| time in 0 minutes, 10 seconds\n","\n","::\n","\n","       Loss: 0.0119(train)     |       Acc: 93.6%(train)\n","       Loss: 0.0000(valid)     |       Acc: 89.6%(valid)\n","\n","\n","Epoch: 3 \\| time in 0 minutes, 9 seconds\n","\n","::\n","\n","       Loss: 0.0069(train)     |       Acc: 96.4%(train)\n","       Loss: 0.0000(valid)     |       Acc: 90.5%(valid)\n","\n","\n","Epoch: 4 \\| time in 0 minutes, 11 seconds\n","\n","::\n","\n","       Loss: 0.0038(train)     |       Acc: 98.2%(train)\n","       Loss: 0.0000(valid)     |       Acc: 90.4%(valid)\n","\n","\n","Epoch: 5 \\| time in 0 minutes, 11 seconds\n","\n","::\n","\n","       Loss: 0.0022(train)     |       Acc: 99.0%(train)\n","       Loss: 0.0000(valid)     |       Acc: 91.0%(valid)        \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QUWyjkc41GV_","colab_type":"text"},"source":["Evaluate the model with test dataset\n","------------------------------------\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"j1BzYycJ1GV_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"8bb57bc7-e094-49b3-ccf2-511c62ca5f90","executionInfo":{"status":"ok","timestamp":1587053921355,"user_tz":-120,"elapsed":157778,"user":{"displayName":"Mehran Jeelani","photoUrl":"https://lh3.googleusercontent.com/-PDuprpGjLik/AAAAAAAAAAI/AAAAAAAACIc/wUSx8bhwcxA/s64/photo.jpg","userId":"00272410087232989111"}}},"source":["print('Checking the results of test dataset...')\n","test_loss, test_acc = test(test_dataset)\n","print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"],"execution_count":28,"outputs":[{"output_type":"stream","text":["Checking the results of test dataset...\n","\tLoss: 0.0002(test)\t|\tAcc: 91.1%(test)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MzW6n_Lb1GWB","colab_type":"text"},"source":["Checking the results of test dataset…\n","\n","::\n","\n","       Loss: 0.0237(test)      |       Acc: 90.5%(test)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"akqqP3NE1GWB","colab_type":"text"},"source":["Test on a random news\n","---------------------\n","\n","Use the best model so far and test a golf news. The label information is\n","available\n","`here <https://pytorch.org/text/datasets.html?highlight=ag_news#torchtext.datasets.AG_NEWS>`__.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"BzA8KuPv1GWC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"335dc370-f4e9-45c4-f7b3-a10a9fee4eeb","executionInfo":{"status":"ok","timestamp":1587054639247,"user_tz":-120,"elapsed":901,"user":{"displayName":"Mehran Jeelani","photoUrl":"https://lh3.googleusercontent.com/-PDuprpGjLik/AAAAAAAAAAI/AAAAAAAACIc/wUSx8bhwcxA/s64/photo.jpg","userId":"00272410087232989111"}}},"source":["import re\n","from torchtext.data.utils import ngrams_iterator\n","from torchtext.data.utils import get_tokenizer\n","\n","ag_news_label = {1 : \"World\",\n","                 2 : \"Sports\",\n","                 3 : \"Business\",\n","                 4 : \"Sci/Tec\"}\n","\n","def predict( model,text, vocab, ngrams):\n","    tokenizer = get_tokenizer(\"basic_english\")\n","    with torch.no_grad():\n","        text = torch.tensor([vocab[token]\n","                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n","        text = text.to(device)\n","\n","        output = model(text, torch.tensor([0],device=device))\n","        return output.argmax(1).item() + 1\n","\n","ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n","    enduring the season’s worst weather conditions on Sunday at The \\\n","    Open on his way to a closing 75 at Royal Portrush, which \\\n","    considering the wind and the rain was a respectable showing. \\\n","    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n","    was another story. With temperatures in the mid-80s and hardly any \\\n","    wind, the Spaniard was 13 strokes better in a flawless round. \\\n","    Thanks to his best putting performance on the PGA Tour, Rahm \\\n","    finished with an 8-under 62 for a three-stroke lead, which \\\n","    was even more impressive considering he’d never played the \\\n","    front nine at TPC Southwind.\"\n","\n","vocab = train_dataset.get_vocab()\n","model = model.to(device)\n","\n","print(\"This is a %s news\" %ag_news_label[predict( model,ex_text_str, vocab, 2)])\n","tokenizer = get_tokenizer(\"basic_english\")\n","essay = [token for token in ngrams_iterator(tokenizer('I love python.'), 2)]\n","print('edday is',essay)\n"],"execution_count":38,"outputs":[{"output_type":"stream","text":["This is a Sports news\n","edday is ['i', 'love', 'python', '.', 'i love', 'love python', 'python .']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"X0V5ctni1GWE","colab_type":"text"},"source":["This is a Sports news\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"ewKHOzVUuKWC","colab_type":"code","colab":{}},"source":["for p in model.fc:\n","  print(p.device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b_6lUytA1GWF","colab_type":"text"},"source":["You can find the code examples displayed in this note\n","`here <https://github.com/pytorch/text/tree/master/examples/text_classification>`__.\n","\n","\n"]}]}